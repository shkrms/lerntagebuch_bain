---
title: "10 Linked Data"
date: 2023-07-16
---
Mit dieser Einheit haben wir den Kurs abgeschlossen und einen kurzen Ausblickblick gehalten, was uns in Zukunft erwarten könnte. Genauer haben wir uns mit Linked-Data-Modellen für Bibliotheken, also mit den Standards Bibframe und Records in Context befasst. Vorgängig haben wir noch ein bisschen mit ChatGPT gespielt, von dem ich anfangs ziemlich begeistert war, dann aber schnell das Interesse verloren habe. Grund dafür war, dass ich viele falsche Informationen bekommen hatte und den Chatbot beim Lügen erwischte. Meine Fragen betrafen jeweils den Garten und damit einhergehende Arbeiten, was eigentlich zu Genüge im Internet behandelt wird. Ich musste ein bisschen lachen, dass der Chatbot im Unterricht die richtige Link-Adresse von OpenRefine ausgegeben hat, weil ich ihn oft nach Links gefragt habe und nur fiktive Adressen zurückbekam. Zudem wurden mir Zusammenstellungen ausgegeben, die ich ganz klar, als falsch identifizieren konnte. Auch das Kürzen und Korrigieren von Texten habe ich schnell aufgegeben, weil ChatGPT nicht sinngemäss kürzt und mir Korrekturen wiedergab, die keine waren, er hat lediglich meine Texte paraphrasiert. Der Ansatz von Linked Data finde ich sehr interessant, da wir im Studium auf verschiedenste Weise mit Ontologien und Semantic Web gearbeitet haben. Das Bibframe auf der Basis von RDF baut, war mir nicht bewusst. Die Aussagen zum Bibframe-Model waren gut nachzuvollziehen, besonders der Vergleich mit MARC21 läuchtete mir ein. So werden in MARC21 viele Angaben doppelt und mehrfach ausgegeben, wie z.B. kurzer Titel, langer Titel o.ä. und hinterlegt diese in eigene Datenfelder. Bibframe ist mit seiner Struktur von Work, Instance und Item sinnvoll aufgesplittet, womit Fehler schneller erkannt und durch die Verknüpfung effektiver beseitigt sind, während im MARC mehrere Datenfelder korrigiert werden müssen. Um die Entwicklungen von Bibframe mitzuverfolgen, wurde uns SWIB empfohlen, die regelmässig Präsentationen zum Thema abhalten. Was mich bei RiC begeisterte ist, dass der Standard wohl fähig ist Informationen zur Provenienz der Medien wiederzugeben. Zudem sind weitere Bezüge mit Relevanz vorgesehen, wie z.B. Events oder Regularien. Für den thematischen Abschluss des Kurses haben wir noch einen kurzen Einblick in das BM-System ALMA gehalten und dann mit der Metadatenanreicherung via OpenRefine und Wikidata abgerundet. Wie in einem früheren Beitrag erwähnt habe ich mit Bibliotheken nicht viel am Hut, daher kann ich nicht mehr dazu sagen, als das ich die Kritik nachvollziehen kann. So wurde erwähnt, dass ALMA über eine grosse Fülle von Funktionen verfügt, jedoch die Handhabung der Software keineswegs besser ist, als bei diverser älterer oder freier Software. Es bleibt daher fragwürdig, ob es Sinn macht, ein solch grosses Softwarepacket einzukaufen, wo man doch die Funktionen verschiedener freier Systeme kombinieren kann. Die Anreicherung von Metadaten mit Normwerten ist bestimmt sinnvoll und scheint mit OpenRefine und Reconciliation einfach von der Hand zu gehen. Die abzugleichenden Daten werden im OpenRefine-Clipboard eingefügt und mit den Funktionen zu Reconciliation auf Wikidata verwiesen. Am besten bekannte Identifier verwenden und Kontrollieren was ausgegeben wird. Es erscheint aber verständlich, dass der Benutzer weiterhin die Kontrolle behalten muss, um Fehler zu vermeiden. Auch das Matchen funktioniert dann besser, weil dem Benutzer eher bekannt ist, in welchem Kontext das Matching erfolgen sollte. Zum Abschluss sei erwähnt, dass mit OpenRefine auch Daten in Wikidata raufgeladen werden können. Dabei haben sie dem Schema von Wikidata zu entsprechen.
