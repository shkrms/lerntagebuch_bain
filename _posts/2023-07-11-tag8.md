---
title: "08 Metadaten modellieren und Schnittstellen nutzen"
date: 2023-07-11
---
#### Schnittstellen, OAI, VuFind-Harvester, Koha, Archive-Space, DSpace, Crosswalks, Schemas, XSLT, MarcEdit
[Lerneinheit 8](https://pad.gwdg.de/1a2uYR-wRziCkvy3RL6gjA#)

Bei der Fülle von Informationen rauchte mir der Kopf, doch die Einheit hat mir sehr gefallen. Ich würde die Session auch nicht kürzen wollen, da hier Zusammenhänge vermittelt wurden, die auseinanderzunehmen Schade wären. Die Einheit festigte das Verständnis sämtlicher Aspekte, die für die bisherigen Lerninhalte massgebend waren und fügte sie in einem Guss zusammen. Die meisten Informationen sind ergänzend ausgesprochen worden, wobei die technische Anwendung im Vordergrund stand. Dementsprechend beschreibt dieser Text die neugewonnenen Erkenntnisse im Bezug zu früher beschriebenen Inhalten. Ausführliche Umschreibungen sind im Lerntagebuch enthalten und bilden die Grundlage. Das [OAI](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag5.html) mit Datenabgleich arbeitet, wurde erwähnt, nun wurde der Einsatz von Z.35.50 und SRU als gezielte Liveabfrage präzisiert. Der Einstieg mit OAI und Koha wurde noch mit selbst angelegten Exemplaren vollzogen, die dann anhand dem Term «?verb=» in der grafischen Oberfläche von Koha gefunden und manipuliert werden konnten. In dieser Einheit wurden Beispieldaten aus dem Dozenten-Repository in die [Linux Arbeitsoberfläche](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag2.html) geladen. Das Laden des OAI-Harvesters, hier VuFind-Harvester, anhand des Befehls erlaubt bereits eine Manipulation des Formats, wobei lediglich das metadataPrefix angepasst werden muss. 

Aufrufen OAI-Harvester: php bin/harvest_oai.php --url=https://koha.adminkuhn.ch/cgi-bin/koha/oai.pl --metadataPrefix=oai_dc test-koha

Nach diesem Vorgehen wurden Beispieldaten von [Koha](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag4.html), [Archive-Space](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag6.html) und [DSpace](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag7.html) in die Arbeitsoberfläche geladen und in eigenen Verzeichnis abgelegt. Im Beispiel oben ist das Verzeichnis Test-Koha am Ende der Befehlszeile ersichtlich. Dieses kann eigenständig angepasst werden, wobei je ein Verzeichnis zu den genannten Systemen geladen und in den Metadaten-Formaten marcxml, oai_ead und oai_dc dargestellt wurde. Durch die grafischen Oberflächen von OAI, aufrufbar über eine BasisUrl, entsprechend dem verwendeten System, ist die Manipulation von Formaten ebenfalls durchführbar. Da wir nach [Schaubild](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag2.html) der Lerninhalte vorgesehen hatten, unterschiedliche Formate zusammenzuführen und gleichwertig in marcxml auszugeben, wurde uns folgend, das Vorgehen von Crosswalks an die Hand gegeben. Crosswalk ist ein gängiger Begriff und beschreibt die Transformation eines Metadaten-Formats in ein anderes. Das sogenannte Mapping bestimmt die Zuordnung der Elemente und Werte, wozu Schemas vorhanden und z.B. bei [LoC](https://www.loc.gov/) gefunden werden. Das gängige Webformat [XSL](https://programminghistorian.org/en/lessons/transforming-xml-with-xsl) entspricht einer W3C Empfehlung und wir haben es als endgültiges Format im Crosswalk angenommen. XSL ist gut lesbar und erlaubt kleine Änderungen im Code, ohne die Struktur nachhaltig zu schädigen. Das Tool [xsltransforms.net](http://xsltransform.net/) gibt uns ein Werkzeug an die Hand um z.B. Daten als DC einzuspeisen und dann anhand eines Schemas, z.B. [DC zu MARCXML](https://www.loc.gov/standards/marcxml/xslt/DC2MARC21slim.xsl), umzuwandeln. Das Tool hat jedoch seine Grenzen, weshalb z.B. eine Transformation von DEA nach MARCXML im [MarcEdit](https://marcedit.reeset.net/) vollzogen wird. MarcEdit hat zwar eine rustikal anmutende Oberfläche, ist aber vollgepackt mit Funktionen zum Bearbeiten des MARCXML und mehr. Eine weitere Möglichkeit zur Transformation ist [OpenRefine](https://shkrms.github.io/lerntagebuch_bain/2023/07/11/tag3.html), wobei ein händischen Modellieren erlaubt und durch einen Templating Exporter gewährleistet wird. Die Vorlage wird nach eigenem Bestimmen angepasst, wobei der Syntax Kompetenz und Geschick erfordert. Beim Transformieren gibt es kein richtiges oder standardisiertes Vorgehen. Es empfiehlt sich die Daten zu analysieren und das Vorgehen entsprechend anzupassen. Zum Validieren der Transformationsergebnisse stehen verschiedene [Tools](https://www.softwarebytes.org/xmlvalidation/) zur Verfügung, obwohl man nicht darüber hinwegkommt, [Schemas](https://www.loc.gov/standards/marcxml/schema/MARC21slim.xsd) durchzugehen und im Ergebnis zu vergleichen. Um das gesamte Vorgehen einzubetten, wurde der ETL-Prozess vermittelt, zu welchem gut nachvollziehbare [Ressourcen](https://it-in-bibliotheken.de/metadaten.html#datenverarbeitung) bereitstehen. Im HedgeDoc sind alternative Tools zur Metadatentransformation einsehbar, worauf zum Schluss der Einheit die Anwendung von JSON-Formaten angesprochen wurde, die eine grosse Verbreitung vorweisen können. Dementsprechend wurden wir mit der JSON-Implementierung durch APIs vertraut gemacht.
